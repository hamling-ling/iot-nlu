{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# local modules\n",
    "from ner_tokenizer_bio import NER_tokenizer_BIO\n",
    "from bert_for_token_classification_pl import BertForTokenClassification_pl\n",
    "\n",
    "# 日本語学習済みモデル\n",
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# インテントの種類数 (None=0, LED_ON=1, LED_OFF=2, READ_THERMO=3, OPEN=4, CLOSE=5, SET_TEMP=6)\n",
    "NUM_INTENT_LABELS = 7\n",
    "\n",
    "# スロットの種類数 (COL=1, COLLTDEV=2, LOC=3, ONOFFDEV=4, OPENABLE=5, TEMPDEV=6, TEMPERTURE_NUM=7, THMDEV=8)\n",
    "NUM_ENTITY_TYPE = 8\n",
    "\n",
    "# データのロード\n",
    "dataset = json.load(open('data/nlp_data.json','r'))\n",
    "\n",
    "# カテゴリーをラベルに変更、文字列の正規化する。\n",
    "for sample in dataset:\n",
    "    sample['text'] = unicodedata.normalize('NFKC', sample['text'])\n",
    "\n",
    "# データセットの分割\n",
    "random.shuffle(dataset)\n",
    "dataset = dataset[:10000]\n",
    "n       = len(dataset)\n",
    "n_train = int(n*0.6)\n",
    "n_val   = int(n*0.2)\n",
    "dataset_train = dataset[:n_train]\n",
    "dataset_val   = dataset[n_train:n_train+n_val]\n",
    "dataset_test  = dataset[n_train+n_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(tokenizer, dataset, max_length):\n",
    "    \"\"\"\n",
    "    データセットをデータローダに入力できる形に整形。\n",
    "    \"\"\"\n",
    "    dataset_for_loader = []\n",
    "    for sample in dataset:\n",
    "        text = sample['text']\n",
    "        entities = sample['entities']\n",
    "        encoding = tokenizer.encode_plus_tagged(\n",
    "            text, entities, max_length=max_length\n",
    "        )\n",
    "        encoding['intent_label'] = sample['intent']\n",
    "        encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
    "        dataset_for_loader.append(encoding)\n",
    "    return dataset_for_loader\n",
    "\n",
    "# トークナイザのロード\n",
    "# 固有表現のカテゴリーの数`num_entity_type`を入力に入れる必要がある。\n",
    "tokenizer = NER_tokenizer_BIO.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_entity_type=NUM_ENTITY_TYPE\n",
    ")\n",
    "\n",
    "# データセットの作成\n",
    "max_length = 128\n",
    "dataset_train_for_loader = create_dataset(\n",
    "    tokenizer, dataset_train, max_length\n",
    ")\n",
    "dataset_val_for_loader = create_dataset(\n",
    "    tokenizer, dataset_val, max_length\n",
    ")\n",
    "\n",
    "# データローダの作成\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train_for_loader, batch_size=32, shuffle=True\n",
    ")\n",
    "dataloader_val  = DataLoader(dataset_val_for_loader, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_data = 2\n",
    "print(dataset_train[i_data])\n",
    "tokens = tokenizer.tokenize(dataset_train[i_data]['text'])\n",
    "print(tokens)\n",
    "print(dataset_train_for_loader[i_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習時にモデルの重みを保存する条件を指定\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=1,\n",
    "    save_weights_only=True,\n",
    "    dirpath='./model',\n",
    ")\n",
    "\n",
    "# 学習の方法を指定\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=5,\n",
    "    callbacks = [checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightningのモデルのロード\n",
    "num_slot_labels = 2*NUM_ENTITY_TYPE+1\n",
    "model = BertForTokenClassification_pl(\n",
    "    MODEL_NAME, num_slot_labels=num_slot_labels, num_intent_labels=NUM_INTENT_LABELS, lr=1e-5\n",
    ")\n",
    "\n",
    "# ファインチューニング\n",
    "trainer.fit(model, dataloader_train, dataloader_val)\n",
    "best_model_path = checkpoint.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(entities_list, entities_predicted_list, type_id=None):\n",
    "    \"\"\"\n",
    "    正解と予測を比較し、モデルの固有表現抽出の性能を評価する。\n",
    "    type_idがNoneのときは、全ての固有表現のタイプに対して評価する。\n",
    "    type_idが整数を指定すると、その固有表現のタイプのIDに対して評価を行う。\n",
    "    \"\"\"\n",
    "    num_entities    = 0 # 固有表現(正解)の個数\n",
    "    num_predictions = 0 # BERTにより予測された固有表現の個数\n",
    "    num_correct     = 0 # BERTにより予測のうち正解であった固有表現の数\n",
    "    indices_incorredt = []\n",
    "    \n",
    "    # それぞれの文章で予測と正解を比較。\n",
    "    # 予測は文章中の位置とタイプIDが一致すれば正解とみなす。\n",
    "    counter = 0\n",
    "    for entities, entities_predicted \\\n",
    "        in zip(entities_list, entities_predicted_list):\n",
    "\n",
    "        if type_id:\n",
    "            entities = [ e for e in entities if e['type_id'] == type_id ]\n",
    "            entities_predicted = [ \n",
    "                e for e in entities_predicted if e['type_id'] == type_id\n",
    "            ]\n",
    "            \n",
    "        get_span_type = lambda e: (e['span'][0], e['type_id'])\n",
    "        set_entities = set( get_span_type(e) for e in entities )\n",
    "        set_entities_predicted = \\\n",
    "            set( get_span_type(e) for e in entities_predicted )\n",
    "\n",
    "        num_entities += len(entities)\n",
    "        num_predictions += len(entities_predicted)\n",
    "        num_correct += len( set_entities & set_entities_predicted )\n",
    "        \n",
    "        # debug\n",
    "        if(len(set_entities) != len( set_entities & set_entities_predicted )):\n",
    "            indices_incorredt.append(counter)\n",
    "        #    print(set_entities)\n",
    "        #    print(set_entities_predicted)\n",
    "\n",
    "        counter += 1\n",
    "    \n",
    "    # 指標を計算\n",
    "    precision = num_correct/num_predictions # 適合率\n",
    "    recall = num_correct/num_entities # 再現率\n",
    "    f_value = 2*precision*recall/(precision+recall) # F値\n",
    "\n",
    "    result = {\n",
    "        'num_entities': num_entities,\n",
    "        'num_predictions': num_predictions,\n",
    "        'num_correct': num_correct,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f_value': f_value\n",
    "    }\n",
    "\n",
    "    print(indices_incorredt)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 性能評価\n",
    "model = BertForTokenClassification_pl.load_from_checkpoint(\n",
    "    best_model_path\n",
    ")\n",
    "model.eval()\n",
    "bert_tc.eval()\n",
    "bert_tc = model.bert_tc.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents_list            = [] # 正解インテントを追加している\n",
    "intents_predicted_list  = [] # 分類されたインテントを追加していく\n",
    "entities_list           = [] # 正解の固有表現を追加していく\n",
    "entities_predicted_list = [] # 抽出された固有表現を追加していく\n",
    "\n",
    "for sample in tqdm(dataset_test):\n",
    "    text = sample['text']\n",
    "    encoding, spans = tokenizer.encode_plus_untagged(\n",
    "        text, max_length=128, return_tensors='pt'\n",
    "    )\n",
    "    encoding = { k: v.cuda() for k, v in encoding.items() } \n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss, logits_intent, logits_slot = bert_tc(**encoding)\n",
    "        scores_intent = logits_intent.cpu().numpy()\n",
    "        scores_slots  = logits_slot[0].cpu().numpy().tolist()\n",
    "\n",
    "    # 分類スコアを固有表現に変換する\n",
    "    entities_predicted = tokenizer.convert_bert_output_to_entities(\n",
    "        text, scores_slots, spans\n",
    "    )\n",
    "    \n",
    "    intents_list.append(sample['intent'])\n",
    "    intents_predicted_list.append(scores_intent.argmax(-1)[0])\n",
    "    entities_list.append(sample['entities'])\n",
    "    entities_predicted_list.append( entities_predicted )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# インテント分類スコア\n",
    "counter = 0.0\n",
    "for pred, truth in zip(intents_predicted_list, intents_list):\n",
    "    counter += float(pred == truth)\n",
    "print('intent classification accuracy = ', counter/len(intents_list))\n",
    "# 固有表現抽出スコア\n",
    "print(evaluate_model(entities_list, entities_predicted_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 個別に実行\n",
    "entities           = [] # 正解の固有表現\n",
    "entities_predicted = [] # 抽出された固有表現\n",
    "\n",
    "# ツンデレを検索\n",
    "#sample = next((x for x in dataset_test if ('いいんだからね' in x['text'])), None)\n",
    "sample = dataset_test[75]\n",
    "\n",
    "text = sample['text']\n",
    "encoding, spans = tokenizer.encode_plus_untagged(\n",
    "    text, max_length=128, return_tensors='pt'\n",
    ")\n",
    "encoding = { k: v.cuda() for k, v in encoding.items() } \n",
    "\n",
    "with torch.no_grad():\n",
    "    total_loss, logits_intent, logits_slot = bert_tc(**encoding)\n",
    "    scores_intent = logits_intent.cpu().numpy()\n",
    "    scores_slots  = logits_slot[0].cpu().numpy().tolist()\n",
    "\n",
    "# Intent 分類スコアを Intent に変換する\n",
    "intent = scores_intent.argmax(-1)[0]\n",
    "# Slot 分類スコアを固有表現に変換する\n",
    "entities_predicted = tokenizer.convert_bert_output_to_entities(\n",
    "    text, scores_slots, spans\n",
    ")\n",
    "\n",
    "print(\"正解\", sample)\n",
    "print(\"予測 intent  :\", intent)\n",
    "print(\"予測 entities:\", entities_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意事項。\"白い\" は 1 token\n",
    "tokens = tokenizer.tokenize('ラボの白いライトオンに')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"./model/iot-nlu-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
